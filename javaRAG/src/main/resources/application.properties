server.port=${PORT:8080}
logging.level.org.atmosphere=warn
spring.mustache.check-template-location=false
vaadin.async-supported=true
vaadin.react.enable=true
vaadin.frontend.hotdeploy=true
# Launch the default browser when starting the application in development mode
vaadin.launch-browser=true
vaadin.allowed-packages=com.vaadin,org.vaadin,dev.hilla,com.vaadin.demo
spring.jpa.defer-datasource-initialization=true
spring.sql.init.mode=always

### JavaRAG config

# Filesystem path to your documentation folder
ai.docs.location=D:/RAG
#TODO:CHANGE THIS TO YOU OWN PATH FOR RAG SOURCE DOCUMENTS


## NOTE: Use ONLY ONE of the two options below at a time.
#TODO:DECIDE WHICH MODEL YOU WANT TO USE AND UNCOMMENT THE CORRESPONDING LINES

# OpenAI API
# Better quality, requires sending data to OpenAI
#langchain4j.open-ai.streaming-chat-model.api-key=REPLACE_THIS_WITH_YOUR_API_KEY
#langchain4j.open-ai.streaming-chat-model.model-name=gpt-4o // CHANGE THIS with the model you want to use


# Local OpenAI compatible API (ollama)
# Not as performant, but your data does not leave your computer

#Uncomment the following lines to use phi3 model
#langchain4j.open-ai.streaming-chat-model.api-key=ollama
#langchain4j.open-ai.streaming-chat-model.base-url=http://localhost:11434/v1
#langchain4j.open-ai.streaming-chat-model.model-name=phi3:latest
#TODO:IF USING Ollama, CHANGE THE ABOVE LINE with the model you want to use, Currently we are using the Micrososft's Phi3 model, phi3:latest is the latest model

#Uncomment the following lines to use llama3 - 8b model
langchain4j.open-ai.streaming-chat-model.api-key=ollama
langchain4j.open-ai.streaming-chat-model.base-url=http://localhost:11434/v1
langchain4j.open-ai.streaming-chat-model.model-name=llama3
#TODO:IF USING Ollama, CHANGE THE ABOVE LINE with the model you want to use, Currently we are using the Meta's Llama3 8b parameters model, phi3:latest is the latest model


#TODO:UNCOMMENT THE FOLLOWING LINES FOR DEBUGGING
# Debug logging to print requests
#logging.level.dev.langchain4j=DEBUG
#logging.level.dev.ai4j.openai4j=DEBUG
#langchain4j.open-ai.streaming-chat-model.log-requests=true
#langchain4j.open-ai.streaming-chat-model.log-responses=true
